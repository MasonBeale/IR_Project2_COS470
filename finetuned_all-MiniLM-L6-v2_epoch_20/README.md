---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:9530
- loss:CosineSimilarityLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: '[TITLE]what mysterious whole number am i[BODY]ullifrom my second
    point of view i copy that amount of firstlilii am the dominant side of two but
    for many im the worstliliill be tricky to find but today might be your dayliliafter
    all my neighbour is 23 of me when you look a certain wayliulblockquote  pwhat
    number am i  it is a whole numberpblockquotephint 1pblockquote classspoiler  p
    the riddle is self referentialpblockquotephint 2 riddle continuationpulliblockquote
    classspoiler  padd me up and i am fairpblockquoteliliblockquote classspoiler  pbuild
    from bits youre not quite there pblockquoteliliblockquote classspoiler  punless
    youve met my friend onlinepblockquoteliliblockquote classspoiler  ptheir position
    is apart of the first linepblockquoteliulphint 3pblockquote classspoiler  p what
    would that amount be referring to why is the phrasing significant whats so special
    about this number anyway what about line 4 remember this riddle could be solved
    without the knowledge of part 2 continuationpblockquotepbonus if you can say what
    number is the neighbour and what number is the friendp'
  sentences:
  - weigh two random coins luckily one coin is heavier than the other could be 11
    g and 10 g 10 g and 9 g 11 g and 9 g take both coins just weighed and put them
    on same side keeping track of the heavier coin and weigh them against another
    two coins you get lucky and both sides weigh the same if if was the 11 g and 10
    g on one side this is impossible if if was the 10 g and 9 g on one side this is
    impossible the only option is for the 11 g and 9 g to be on the same side the
    heavier coin that you kept track of is 11 g the other is 9 g therefore 2 lucky
    weighings is the ideal case
  - the numbers relevant to this riddle are 1351213 with the expected answer being
    13 the neighbour is 11 the friend is 5 the riddle is all about the properties
    of the number and using digits general theme of 3 explanation the riddle is deceitful
    in that you cannot use any information from line 1 from my second point of view
    i copy that amount of first  until you know what the number is or have narrowed
    it down to a point where you can use process of elimination the ideal place to
    start is line 2 i am the dominant side of two but for many im the worst  which
    can be misinterpreted for many ideas like literally being next to 2 but it is
    referring to pythagorean triples where 13 is the hypotenuse of 5 and 12 the second
    part of line 2 can mean many things such as 13 being almost the smallest positive
    integer hypotenuse or that it is leading into line 3 for example but the key point
    is that 13 is seen as an unlucky number for many people line 3  ill be tricky
    to find but today might be your day  refers to the surprising difficulty of this
    riddle but also the struggle with prime numbers and the strangeness of the number
    13 this riddle was posted on 12th jan from where i am but it was 13th jan elsewhere
    which was the hint in the second half of line 3 but it was also implying that
    today could be your unlucky day now that we know that the number could be 13 we
    can then interpret line 1 the point of view was referring to number bases and
    the second digit of thirteen is 3 so converting 13 from base 10 to base 3 we see
    that it becomes 111 which just so happens to be 3 copies of the first digit 1
    line 4  after all my neighbour is 23 of me when you look a certain way  purposefully
    is a continuation of line 3 that refers to primes the prime number before 13 is
    11 and counting the number of 1s we see that in comparison to 111 it is 23 explanation
    of hint 2 continuation this hint was used to clarify what the riddle was about
    since you may not be in the right mindset to get the answer line 1  add me up
    and i am fair  was used to narrow everything down so that you dont get too carried
    away on the wrong track it was referring to the summation of the digits in 13
    being the even number 4 but also the summation of the sides of the 51213 triangle
    line 2  build from bits youre not quite there  simply was to make you think about
    number bases binary is associated with bits and is in base 2 but the riddle required
    you to use base 3 line 3  unless youve met my friend online  is relating back
    to line 2 and makes an actual case for building from bits as length 13 in the
    triangle is the result of the two smaller lines being added together resultant
    online has a double meaning but in both cases it is a play on words it refers
    to the smaller lines in the triangle defining 13 but it also refers to the prime
    number sequence that 5 from the triangle just so happens to sit on line 4  their
    position is apart of the first line  takes the number 5 and asks you what its
    position in the prime number sequence is it is third and the number 3 is used
    at the start of the riddle
  - answer you could be dead or drunk  both are perfectly normal things for a man
    but the pilot wont go forward with you in that state today  if youre dead you
    have to be stowed not sitting in the seat  you could sit there when youre sober
    or alive but not today also you could just be a passenger sitting in the wrong
    seat  as some airplines insist people are correctly seated for take off so they
    can identify missing people or bodies in a crash  you dont need to be anyone particular
    and you could have had this seat on another day but today your seat is another
    seat so until you stop being annoying and move to your allocated seat for takeoff
    the pilot cant move onto the runway for taxiing or takeoffdespite having clearance
    from the tower to do so
- source_sentence: '[TITLE]i am found everywhere and yet nowhere[BODY]pi am the posterchild
    for what the buddhists believe yet they dont worship meppmy cycle is eternal yet
    death follows meppsome see me as a wish granter and my opposite slumbers beneathppif
    i cry for you you will be relievedppwhat am ipphintpblockquote classspoiler  p
    i am strongalivestrong until i briefly am notpblockquotephint 2pblockquote classspoiler  p
    line 3 each line is a reference to an aspect of me can you catch em allpblockquotephint
    3pblockquote classspoiler  p line 4 bravely fought the tears saved i was also
    secretly the escape from the chamberpblockquote'
  sentences:
  - he said to switch their horses the condition was that whose horse would finish
    last not them so they exchanged horses and raced to their full potential
  - im not a native english speaker but ill do my bestis it a phoenix i am the posterchild
    for what the buddhists believe yet they dont worship me im thinking of reborn
    my cycle is eternal yet death follows me phoenix dies in flames but is reborn
    from its ashes if i cry for you you will be relieved phoenix tears are known to
    be a healing substance
  - i think your friend stayed up all night looking at a constellation reasoning if
    each word maps to a letter then there must be 13 letters in the answer and constellation
    is the only 13letter word i can find which fits the theme of astronomy its also
    possible that he spent the night gazing at some bachelorettes ive come to this
    view because of the dastardliness of the puzzle
- source_sentence: '[TITLE]crack this rhyme[BODY]blockquote  pemyou can see me inside
    barsembr  emand also inside the starsembr  emsome may call me a keyembr  emthe
    above line is a clue to theeembr  emmy brothers are bigger than meembr  emi again
    say the above line is a clue to theeembr  emim found inside a packembr  emgo ahead
    and try to crackempblockquotepwho am ip'
  sentences:
  - the first two lines make me think plasma because it is in bars as in taverns in
    the form of plasma televisions at least in name and it is in stars literally as
    a state of matter not entirely sure of the other lines yet is it plasma
  - i feel like this has something to do with gaussian rounding  since the town is
    named gauss  the numbers are being rounded up or down to one significant digit
    like hexominos answer  in gaussian rounding numbers ending in 5 are rounded to
    the nearest even number in the previous digit  but that doesnt fit with the vending
    machine so it may be that in this town i think one of the following is happening
    1 numbers ending in 5 are rounded toward the nearest odd number instead 2 numbers
    are rounded toward the nearest odd number no matter what the trailing digits are  for
    example 19 would round to 10 there arent enough examples to distinguish between
    the two cases above the meals 12 rounds down to 10 the gas 28 rounds up to 30
    the hotel 270 rounds up to 300 the vending machine 025 rounds to 03 and three
    quarters make 09 which covers the cost of the chocolate which was 075 which might
    be rounded down to 07
  - gryffindors strategy will be to take the lowest number in the hufflepuff urn every
    time and it will work because every marble will be guaranteed to be removed at
    most n turns after you put it in where n is the number on the marble if a marble
    is not put in it will not be in at the end if a marble is put in then it will
    be removed and it will not be in therefore no marbles will be in hufflepuff at
    the end
- source_sentence: '[TITLE]packing pentominoes in a circle[BODY]pyou want to prepare
    a pizza of 12 flavors  you have 12 oddlyshaped pieces of cheese that you decide
    to use for the pizza  the shapes happen to be ppoh well forget it  this isnt going
    to be even remotely realistic  so here is the problemppi was playing with pentominoes
    and figured you can pack them nicely in a circle of radius 5ppa hrefhttpsistackimgurcom3n3ggpng
    relnoreferrerimg srchttpsistackimgurcom3n3ggpng altenter image description here
    appthis immediately cries for the question is this optimal  if not what is the
    radius of the smallest circle that can accommodate all 12 pentominoes inside without
    overlapshow an arrangement that minimizes the radiusppspoiler alert the picture
    above is emnotem optimalppscoreboardbr 484323  loopy waitbr 486594  florian fbr
    488966  ravi fernandobr 492443  daniel mathiasbr 494975  capbr 498189  franciszek
    reminp'
  sentences:
  - this is more of a brain dump at the moment but hopefully it can help someone the
    images are named as in the alt text as follows in order instruction1 instruction2
    instruction3 bottles scale code table keys  labels  this if not anything else
    might indicate that the last image is a key for the second last  the color at
    the top indicates for which columns the colors are a key and the left column of
    colors is a key for the left column of the code and ditto for the right the bottles  labels  image
    has a series of 8x8 grids with holes in  this most likely corresponds to the 8x8
    grid of symbols in the instruction1 image how i dont know note in the instruction1
    image at the top left there are 6 bottles not colored the same i have labelled
    them starting from the most blue these correspond to the grid coordinates c2 e4
    a1 b4 f1 and a3 respectively these also might correspond to the six colors in
    the code in the sacle image i did not get anything useful by overlaying the holed
    grids that correspond to those coordinates onto the symbol grid but maybe i just
    didnt notice a pattern heres a transcription of the code table image whogtx fvtojrossiu
    zthxicpynnw etugdenfiz ttoemwavbaej uzrzfiwvtgye zhkxprmihjz rwulblefdts zogzefssufy
    wleyaziuxkxt vtpledtkvate aoyyufohib sghrzmekglm pmeadp heres a frequency analysis
    odd how there are so many zs e  t  z  f  i  o  u  a  g  h  s  w  y  l  m  p  r  v  x  d  k  b  j  n  c  q12
    12 10  7  7  7  7  6  6  6  6  6  6  5  5  5  5  5  5  4  4  3  3  3  1  0 the
    puzzle might even be able to be broken with cryptanalysis of the code in the keys
    image the colors in the left and right columns are always the same just in a different
    order this would lead me to conjecture that one must use this color key to reorder
    the corresponding column in the code table image after which something will be
    spelled out these colors also correspond to the ones on the scale in the scale
    image this is all ive got for now
  - the smallest radius is apparently less than frac12sqrt9242frac12sqrt97approx49244
    one such arrangement is shown here
  - trick question ice cube trays are designed to stack on top of each other without
    freezing together there are pieces on the bottoms of the trays that sit on the
    top lip of the tray below it so they dont sink in  freeze together so you can
    put all 7 trays in at once 12 cubes per tray x 7 trays x 4 batches per hour  336
    ice cubes per hour
- source_sentence: '[TITLE]as stack exchange does[BODY]blockquotepyou may call me
    fakeppbut with great power i emmakeemppand i keep you awakeppfrom a stack exchange
    site my name i useppbe careful with your commands dont abusepblockquotepwhat am
    ip'
  sentences:
  - update this answer is partlymostly wrong see rexs answer but its not bad for a
    wild guess i feel the set of possible formulae is infinite but because its countably
    infinite the second angel can keep counting until hesheit arrives at the answer  the
    key point to my mind being that although there are an infinite number of possibilities
    for the first angel to choose from once theyve chosen a specific one it takes
    a finite amount of time for angel 2 to count up until they get to it  which they
    dont have to go through all the infinite possibilities to do for any finite known
    set of mappings inputs to outputs there can always be more than one function which
    could apply so if the angels are partway through the game and have a set of mappings
    a b c     d e f  then that isnt enough information for the second angel to know
    that there can only be one possible function that could produce that mapping so
    the second angel will need to ask for more information but no amount of information
    will ever be enough for total certainty because there will always be more than
    one possible function that could apply alternatively the second angel could guess
    a function without being totally certain that theyre right but then it would be
    possible for them  by chance  to always guess wrongly so the game could still
    go on forever so the angels shouldnt play the game  they should watch england
    lose at cricket instead because that always happens within a finite time one way
    to prove me wrong here would be to come up with a set of mappings inputs to outputs
    which can only be produced by one single function  i dont think that any such
    exists if it does then im wrong and the angels should play the game example the
    sequence is 1 2 3 4 5  1 2 3 4 5 this function could apply fx  x but then so could
    this one fx  x  6  x  2x and so could any function in this infinite set of functions
    where k is any integer fx  x  6  x  kx
  - possibility 1  email you may call me fake fake email which is not authentic but
    with great power i make it can be used as a formal communication and i keep you
    awake by waiting for them reading and writing them title user needs to have one
    possibility 2  community you may call me fake could be considered fake since it
    is online and users dont really need each other but with great power i make well
    moderators have power and highly involved members can even put that in their cvs
    and i keep you awake by being involved in questions answers and chat title stack
    exchange is all about community
  - ra notation where r iv vii xii capacity a 012 filling informational only  direction
    of flow xii12vii0 vii7iv0 iv4xii5 vii3iv0 xii8vii0 vii7iv3 and finally iv4xii2
    this leaves us with xii6 and vii6
pipeline_tag: sentence-similarity
library_name: sentence-transformers
metrics:
- pearson_cosine
- spearman_cosine
model-index:
- name: SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2
  results:
  - task:
      type: semantic-similarity
      name: Semantic Similarity
    dataset:
      name: Unknown
      type: unknown
    metrics:
    - type: pearson_cosine
      value: 0.16907367513693866
      name: Pearson Cosine
    - type: spearman_cosine
      value: 0.1717924232078548
      name: Spearman Cosine
---

# SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9 -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the 🤗 Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    '[TITLE]as stack exchange does[BODY]blockquotepyou may call me fakeppbut with great power i emmakeemppand i keep you awakeppfrom a stack exchange site my name i useppbe careful with your commands dont abusepblockquotepwhat am ip',
    'possibility 1  email you may call me fake fake email which is not authentic but with great power i make it can be used as a formal communication and i keep you awake by waiting for them reading and writing them title user needs to have one possibility 2  community you may call me fake could be considered fake since it is online and users dont really need each other but with great power i make well moderators have power and highly involved members can even put that in their cvs and i keep you awake by being involved in questions answers and chat title stack exchange is all about community',
    'ra notation where r iv vii xii capacity a 012 filling informational only  direction of flow xii12vii0 vii7iv0 iv4xii5 vii3iv0 xii8vii0 vii7iv3 and finally iv4xii2 this leaves us with xii6 and vii6',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

## Evaluation

### Metrics

#### Semantic Similarity

* Evaluated with [<code>EmbeddingSimilarityEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.EmbeddingSimilarityEvaluator)

| Metric              | Value      |
|:--------------------|:-----------|
| pearson_cosine      | 0.1691     |
| **spearman_cosine** | **0.1718** |

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset


* Size: 9,530 training samples
* Columns: <code>sentence_0</code>, <code>sentence_1</code>, and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                           | sentence_1                                                                          | label                                                          |
  |:--------|:-------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|:---------------------------------------------------------------|
  | type    | string                                                                               | string                                                                              | float                                                          |
  | details | <ul><li>min: 31 tokens</li><li>mean: 167.88 tokens</li><li>max: 256 tokens</li></ul> | <ul><li>min: 8 tokens</li><li>mean: 114.94 tokens</li><li>max: 256 tokens</li></ul> | <ul><li>min: 0.0</li><li>mean: 0.36</li><li>max: 1.0</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | sentence_1                                                                                                                                                                                                                                                                                                                                            | label            |
  |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|
  | <code>[TITLE]why wont the plane start[BODY]pi enter a perfectly normal airplane at a perfectly normal airport and i sit down in a perfectly normal seat however so long as i am sitting in this seat the plane will not start im not in the pilots or copilots seat and as i am a perfectly normal man i could definitely be in this seat but apparently not today im not in an emergency aisle everyone is on board including the flight attendants passengers pilot and copilot there is nothing obstructing the plane and the runway but as long as i sit in this particular seat the plane will not startppstrongwhy wont the plane startstrongphrpstrongclarificationsstrongpblockquote classspoiler  padditionally the plane is cleared for take off there is nothing wrong with anything outside of the plane or with the plane itself everyone who is supposed to be in the plane is inside the plane and no one else is im not a child and im in perfectly good health the pilotcrew are free to get to wherever they want and the ai...</code> | <code>you are a baggage handler sitting in a perfectly normal seat possibly a nice armchair which has been loaded into the cargo hold the plane cant take off whilst you are there but if the seat was in you living room youd be fine youre allowed onto the plane to load the baggage but dont have a ticket</code>                                 | <code>1.0</code> |
  | <code>[TITLE]my wife is lost my gps is lost where are we[BODY]pdarling can you please turn on the gps i asked my wifepphang on i know exactly where we are she repliedppcan you for once listen to meppshe ignored my remark immersed in the reading of the map this was not the first time we were going on a trip and this was definitely not the first time we were having a heated discussion because of this argh in moments like these i hate her for not listening to me especially when we are in a city we dont knowppseriously you never knew how to read a map turn on the damn thingpphey chill look you just have to turn right hereppi turned rightppto the right again dont miss itppi turned right again but i smelt something was wrongppok seems that you have to turn rippagain were back on the same road you cant read a map i already told youppshe fulminated and turned on the gps yes she should have done that in the first place rightpblockquotepin 100 meters turn right then in 100 meters turn leftpblockquotep...</code> | <code>you are in space explanation  the movement done with the car is similar to the one done with the spool while sewing with a back and forth at the end to close the string another name for the spool used for sewing is shuttle  hence you are in space</code>                                                                                   | <code>0.0</code> |
  | <code>[TITLE]getting on on the trend train  riley riddle[BODY]blockquote  pmy first is fruitbr  last is knuckle substitutebr  middle one is space so smallbr  but i am ignorant overallpblockquotepnote  onlyf got the first part correctppthe final word does not have any abbreviation or short cut it uses word as it is pphint pblockquote  pme and my friend were playing the guess me game  this is how questioning and answering went p    pumm fruit    yesbr  easily available fruit   yesbr  related to home  in a way yesbr  what about drinks  yaasssbr  daiquiri  nobr  martini  no but closebr  superhero   yesbr  from dc   nobr  marvel then  nopepblockquote</code>                                                                                                                                                                                                                                                                                                                                                                     | <code>i guess the answer is apprentice my first is fruit app refers to apple i suppose last is knuckle substitute beaten by knuckles can be hurt yet ice may also do the trick middle one is space so small usually a rent place cant be big but i am ignorant overall an apprentice is ignorant of the technique that heshe is going to learn</code> | <code>1.0</code> |
* Loss: [<code>CosineSimilarityLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss) with these parameters:
  ```json
  {
      "loss_fct": "torch.nn.modules.loss.MSELoss"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 1
- `fp16`: True
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: True
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: False
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch  | Step | Training Loss | spearman_cosine |
|:------:|:----:|:-------------:|:---------------:|
| 0.8389 | 500  | 0.2433        | -               |
| 1.0    | 596  | -             | 0.1496          |
| 0.8389 | 500  | 0.2212        | -               |
| 1.0    | 596  | -             | 0.1744          |
| 0.8389 | 500  | 0.2134        | -               |
| 1.0    | 596  | -             | 0.1861          |
| 0.8389 | 500  | 0.2073        | -               |
| 1.0    | 596  | -             | 0.1900          |
| 0.8389 | 500  | 0.2015        | -               |
| 1.0    | 596  | -             | 0.1916          |
| 0.8389 | 500  | 0.1957        | -               |
| 1.0    | 596  | -             | 0.1928          |
| 0.8389 | 500  | 0.1896        | -               |
| 1.0    | 596  | -             | 0.1933          |
| 0.8389 | 500  | 0.1833        | -               |
| 1.0    | 596  | -             | 0.1941          |
| 0.8389 | 500  | 0.1767        | -               |
| 1.0    | 596  | -             | 0.1944          |
| 0.8389 | 500  | 0.1699        | -               |
| 1.0    | 596  | -             | 0.1947          |
| 0.8389 | 500  | 0.1628        | -               |
| 1.0    | 596  | -             | 0.1933          |
| 0.8389 | 500  | 0.1555        | -               |
| 1.0    | 596  | -             | 0.1908          |
| 0.8389 | 500  | 0.148         | -               |
| 1.0    | 596  | -             | 0.1862          |
| 0.8389 | 500  | 0.1404        | -               |
| 1.0    | 596  | -             | 0.1832          |
| 0.8389 | 500  | 0.1328        | -               |
| 1.0    | 596  | -             | 0.1786          |
| 0.8389 | 500  | 0.1253        | -               |
| 1.0    | 596  | -             | 0.1711          |
| 0.8389 | 500  | 0.1181        | -               |
| 1.0    | 596  | -             | 0.1733          |
| 0.8389 | 500  | 0.1111        | -               |
| 1.0    | 596  | -             | 0.1756          |
| 0.8389 | 500  | 0.1035        | -               |
| 1.0    | 596  | -             | 0.1720          |
| 0.8389 | 500  | 0.0974        | -               |
| 1.0    | 596  | -             | 0.1718          |


### Framework Versions
- Python: 3.10.12
- Sentence Transformers: 3.3.0
- Transformers: 4.46.2
- PyTorch: 2.5.1+cu121
- Accelerate: 1.1.1
- Datasets: 3.1.0
- Tokenizers: 0.20.3

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->